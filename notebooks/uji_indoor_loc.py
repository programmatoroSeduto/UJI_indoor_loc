# -*- coding: utf-8 -*-
"""UJI_INDOOR_LOC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tb7J0M1kIaXJf3QfgsFZa3Rd-k_kQY0a

# UJI Indoor Localization with SVR

*Project for Machine Learning 2 course*

## TODO

- visual dati: distribuzione longitude latitude
- salvataggio del modello in CSV. Salvare per entrambi i modelli `lm_lat` e `lm_long` i coefficienti della learning machine in `.dual_coef_` e le ennuple del training set corrispondenti agli indici contenuti in `.support_`. 
- salvare i parametri con cui sono stati fatti i learning (i best parameter)
"""

from google.colab import drive
drive.mount('/content/drive')

# === Settings

# percorso del dataset
data_source_idx = 2

# imposta un limite per lo screen usando NumPy
limit_print = True

# salvare o no i risultati del learning?
save_res = False

# === Imports

# frameworks vari
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import pandas as pd

# toy samples
import sklearn.datasets as datasets

# sciKitLearn
import sklearn.model_selection as model_selection
import sklearn.preprocessing as preprocessing
import sklearn.svm as svm
import sklearn.neighbors as neighbors
import sklearn.metrics as metrics

# path handler
from pathlib import Path 

# per il salvataggio del modello su file
import pickle as pk

# === Setting Application

# data path select
if data_source_idx == 0 :
  # da sample_data
  print( "PATH: ", "sample" )
  training_set_path = '/content/sample_data/UJI_indoor_loc/TrainingData.csv'
  test_set_path = '/content/sample_data/UJI_indoor_loc/ValidationData.csv'
  save_path = '/content/sample_data/UJI_indoor_loc/'

elif data_source_idx == 1 :
  # dal drive di fra
  print( "PATH: ", "fra" )
  training_set_path = '/content/drive/MyDrive/UJI_indoor_loc/UJI_indoor_loc/TrainingData.csv'
  test_set_path = '/content/drive/MyDrive/UJI_indoor_loc/UJI_indoor_loc/ValidationData.csv'
  save_path = '/content/drive/MyDrive/UJI_indoor_loc/UJI_indoor_loc/'

elif data_source_idx == 2 : 
  # dal drive di fede
  print( "PATH: ", "fede" )
  training_set_path = '/content/drive/MyDrive/UJI_indoor_loc/UJI_indoor_loc/TrainingData.csv'
  test_set_path = '/content/drive/MyDrive/UJI_indoor_loc/UJI_indoor_loc/ValidationData.csv'
  save_path = '/content/drive/MyDrive/UJI_indoor_loc/UJI_indoor_loc/'

else:
  print( "data source idx not found." )
  training_set_path = None
  test_set_path = None

# limit for printing in numPy
if limit_print:
  np.set_printoptions(threshold=30)
else:
  np.set_printoptions(threshold=np.inf)

"""# Toy Sample per SVR

SVR: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html

how to evaluate a regression model: https://machinelearningmastery.com/regression-metrics-for-machine-learning/

"""

'''

# diabets dataset
# -> https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset

X, y = datasets.load_diabetes( return_X_y = True )
print( type(X) , " --- ", X.shape )
print( type(y) , " --- ", y.shape )

# normalizzazione
scaleX = preprocessing.MinMaxScaler( )
X = scaleX.fit_transform( X )

# shuffle
idx = np.random.permutation( range(0, X.shape[0]) )
print( type(idx), " -- ", idx )
X, y = X[idx, :], y[idx]

# split del dataset in training e validation
n_tr_perc = .5
n_tr = int(X.shape[0] * n_tr_perc)
print( n_tr )
X_tr = X[ 0:n_tr, : ]
y_tr = y[ 0:n_tr ]
X_tt = X[ (n_tr+1):, : ]
y_tt = y[ (n_tr+1): ]

# SVR model selection
grid = { 
	'C': np.logspace( -4, 3, 15 ),
	'kernel' : ['rbf'],
	'gamma' : np.logspace(-4, 3, 10),
	'epsilon' : [0, 0.01]  
}
lm_model = model_selection.GridSearchCV( 
	estimator  = svm.SVR( ),
	param_grid = grid,
	scoring    = 'neg_mean_absolute_error',
	cv         = 10,
	verbose    = 2
)
H = lm_model.fit( X_tr, y_tr )
print( "--- Best Params ---" )
print( "C : ", H.best_params_['C'] )
print( "kernel : ", H.best_params_['kernel'] )
print( "gamma : ", H.best_params_['gamma'] )
print( "epsilon : ", H.best_params_['epsilon'] )
print( "---" )

# SVR
lm = svm.SVR( 
  C = H.best_params_['C'],
	kernel = H.best_params_['kernel'],
	gamma = H.best_params_['gamma'],
	epsilon = H.best_params_['epsilon'] 
)
lm.fit( X_tr, y_tr )

# prediction
ym = lm.predict( X_tt )
print( np.vstack( (ym, y_tt) ).T )

# regression results with scatter plot
print( "MSE: ", metrics.mean_squared_error( ym, y_tt, squared = False ) )
print( "Score of the learner: ", lm.score( X_tt, y_tt ) )
fig, ax = plt.subplots( )
ax.plot( y_tt, ym, 'xr' )
'''

"""# Toy Sample per kNeighborRegressor"""

'''

# diabets dataset
# -> https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset

X, y = datasets.load_diabetes( return_X_y = True )
print( type(X) , " --- ", X.shape )
print( type(y) , " --- ", y.shape )

# normalizzazione
scaleX = preprocessing.MinMaxScaler( )
X = scaleX.fit_transform( X )

# shuffle
idx = np.random.permutation( range(0, X.shape[0]) )
print( type(idx), " -- ", idx )
X, y = X[idx, :], y[idx]

# split del dataset in training e validation
iterations = [0.1 , 0.2, 0.3, 0.4, 0.5 ]
for i in range(len(iterations)):
	n_tr_perc = i
	n_tr = int(X.shape[0] * n_tr_perc)
	print( n_tr )
	X_tr = X[ 0:n_tr, : ]
	y_tr = y[ 0:n_tr ]
	X_tt = X[ (n_tr+1):, : ]
	y_tt = y[ (n_tr+1): ]

n_tr_perc = 0.1
n_tr = int(X.shape[0] * n_tr_perc)
print( n_tr )
X_tr = X[ 0:n_tr, : ]
y_tr = y[ 0:n_tr ]
X_tt = X[ (n_tr+1):, : ]
y_tt = y[ (n_tr+1): ]

#  model selection for kNR
grid = { 
	'n_neighbors' : np.arange( 1, 5, 1 ),
  'weights'     : ['uniform', 'distance'],
  'leaf_size'   : np.arange( 1, 10, 1 )
}
lm_model = model_selection.GridSearchCV( 
	estimator  = neighbors.KNeighborsRegressor( ),
	param_grid = grid,
	scoring    = 'neg_mean_absolute_error',
	cv         = 10,
	verbose    = 2
)
H = lm_model.fit( X_tr, y_tr )
print( "--- Best Params ---" )
print( "n_neighbors : ", H.best_params_['n_neighbors'] )
print( "weights : ", H.best_params_['weights'] )
print( "leaf_size : ", H.best_params_['leaf_size'] )
print( "---" )

# kNR
lm = neighbors.KNeighborsRegressor( 
  n_neighbors = H.best_params_['n_neighbors'],
	weights = H.best_params_['weights'],
	leaf_size = H.best_params_['leaf_size']
)
lm.fit( X_tr, y_tr )

# prediction
ym = lm.predict( X_tt )
print( np.vstack( (ym, y_tt) ).T )

# regression results with scatter plot
print( "MSE: ", metrics.mean_squared_error( ym, y_tt, squared = False ) )
print( "Score of the learner: ", lm.score( X_tt, y_tt ) )
fig, ax = plt.subplots( )
ax.plot( y_tt, ym, 'xr' )

'''

"""# Learning sul dataset finale"""

# Commented out IPython magic to ensure Python compatibility.
# %pylab inline
mpl.rcParams['figure.figsize'] = (15, 6)

"""## Caricamento dei dati"""

# read from file
Ds_tr = np.random.permutation(pd.read_csv( training_set_path ).to_numpy( )[:, list(range(0, 520)) + [520, 521]])
Ds_tt = np.random.permutation(pd.read_csv( test_set_path ).to_numpy( )[:, list(range(0, 520)) + [520, 521]])

# limita il numero di righe
perc_tr = .025
perc_tt = .05
n_tr = int( Ds_tr.shape[0] * perc_tr )
n_tt = int( Ds_tt.shape[0] * perc_tt )
Ds_tr = Ds_tr[0:n_tr, :]
Ds_tt = Ds_tt[0:n_tt, :]

# quanti dati
print( "Samples in Ds_tr : ", Ds_tr.shape[0] )
# print( "columns in Ds_tr : ", Ds_tr.shape[1] )
print( "Samples in Ds_tt : ", n_tt )

# Salvataggio dati
if save_res:
  # coversione in dataframe 
  df_tr = pd.DataFrame(Ds_tr)
  df_tt = pd.DataFrame(Ds_tt)

  print( "cols of df_tr : ", df_tr.shape[1] )

  # salvataggio dati in foramto esportabile csv
  df_tr.to_csv( save_path + 'myTrainingSet.csv', index=False )
  df_tt.to_csv( save_path + 'myTestSet.csv', index=False )

"""## Data Visualization (elementare)

### Print semplice delle coppie (lat, long)
"""

fig1, ax1 = plt.subplots( )

# Ds_tr = pd.read_csv( save_path + 'myTrainingSet.csv' ).to_numpy( )
# print( "cols of Ds_tr : ", Ds_tr.shape[1] )

ax1.grid( True )
ax1.plot( Ds_tr[:, 520], Ds_tr[:, 521], 'x' ) # longitudine - latitudine

plt.show( )

"""## Normalizzazione e preparazione dati"""

# separazione del dataset in due parti
X_tr = Ds_tr[:, 0:520]
y_tr = Ds_tr[:, [520, 521]]
X_tt = Ds_tt[:, 0:520]
y_tt = Ds_tt[:, [520, 521]]

# normalizzazione
scaleX = preprocessing.MinMaxScaler( feature_range=(0, 1) )
scaleX.fit( np.row_stack((X_tr, X_tt)) )

X_tr = scaleX.transform( X_tr )
X_tt = scaleX.transform( X_tt )

# Salvataggio dati

# coversione in dataframe 
# df_X_tr = pd.DataFrame(X_tr)
# df_X_tt = pd.DataFrame(X_tt)

# salvataggio dati in foramto esportabile csv
# df_X_tr.to_csv(save_path + "normalised X_tr")
# df_X_tr.to_csv(save_path + "normalised X_tt")

# salva lo scaler
if save_res:
  with open( save_path + 'scaler.sav', "wb" ) as fil:
    pk.dump( scaleX, fil )

"""## Model Selection -- SVR

Tipi di Kernel, vedi [user uide SVR](https://scikit-learn.org/stable/modules/svm.html#svm-regression):

C per "valori bassi" -- stiamo ipotizzando tanto rumore nelle osservazioni 

C è 1 di default (scelta ragionevole in molti casi), diminuendo C aumentiamo la regolarizzazione (C inversamente proporzionale a lambda) quindi tendiamo a fidarci poco dei dati

Più C cresce, più la regolarizzazione diminuisce (tendiamo ad una "ERM" con kernel gaussiano) e quindi tendiamo a fidarci di più dei dati. 

Facciamo una prova bilanciata: un po' di punti > 1 e un po' di punti < 1 in modo da capire dal tuning se i dati sono o no tanto affetti da rumore. 

*giustificazione per il tuning di gamma?* A caso. 

*giustificazione per epsilon?* un E=.01 corrisponde ad un errore consentito di distanza rispetto al punto esatto pari a massimo 1cm di distanza rispetto ad una superficie di approssimativamente 600m^2. 

"""

svr_param = {
	'C'       : np.logspace( -4, 3, 7 ),
	'gamma'   : np.logspace( -4, 3, 6 ),
	'epsilon' : [0, 0.01]
}

"""### Model Selection per la longitudine"""

# Model Selection per SVR -- longitudine
H_long = model_selection.GridSearchCV( 
	estimator  = svm.SVR( kernel='rbf' ),
	param_grid = svr_param,
	scoring    = 'neg_mean_absolute_error',
	cv         = 2,
	verbose    = 2
).fit( X_tr, y_tr[:, 0] )

# parametri del modello
print( "--- Best Params for Longitude ---" )
print( "C : ", H_long.best_params_['C'] )
# print( "kernel : ", H_long.best_params_['kernel'] )
print( "gamma : ", H_long.best_params_['gamma'] )
print( "epsilon : ", H_long.best_params_['epsilon'] )
print( "---" )

# salvataggio dei coefficienti per la longitudine
# pd.DataFrame( H_long ).to_csv( save_path + "params_train_long.csv" , index=False)

"""### Model Selection per la latitudine"""

# Model Selection per SVR -- latitudine
H_lat = model_selection.GridSearchCV( 
	estimator  = svm.SVR( kernel='rbf' ),
	param_grid = svr_param,
	scoring    = 'neg_mean_absolute_error',
	cv         = 2,
	verbose    = 2
).fit( X_tr, y_tr[:, 1] )

# parametri del modello
print( "--- Best Params for Latitude ---" )
print( "C : ", H_lat.best_params_['C'] )
# print( "kernel : ", H_lat.best_params_['kernel'] )
print( "gamma : ", H_lat.best_params_['gamma'] )
print( "epsilon : ", H_lat.best_params_['epsilon'] )
print( "---" )

# salvataggio dei coefficienti per la latitudine
# pd.DataFrame( H_lat ).to_csv( save_path + "params_train_lat.csv" )

"""## Training SVR"""

# SVR su longitudine
lm_long = svm.SVR( 
  C = H_long.best_params_['C'],
	#kernel = H_long.best_params_['kernel'],
	kernel = 'rbf',
	gamma = H_long.best_params_['gamma'],
	epsilon = H_long.best_params_['epsilon'] 
).fit( X_tr, y_tr[:, 0] )
print( "--- Score Longitudine --- " )
print( "Score sul training set: ", lm_long.score( X_tr, y_tr[:, 0] ) )
# print( "Score sul test set: ", lm_long.score( X_tt, y_tt[:, 0] ) )
print( "---" )

# SVR su latitudine
lm_lat = svm.SVR( 
  C = H_lat.best_params_['C'],
	#kernel = H_long.best_params_['kernel'],
	kernel = 'rbf',
	gamma = H_lat.best_params_['gamma'],
	epsilon = H_lat.best_params_['epsilon'] 
).fit( X_tr, y_tr[:, 1] )
print( "--- Score Latitudine --- " )
print( "Score sul training set: ", lm_lat.score( X_tr, y_tr[:, 1] ) )
# print( "Score sul test set: ", lm_lat.score( X_tt, y_tt[:, 1] ) )
print( "---" )

"""## Test del modello SVR

### Output della LM
"""

ym_long = lm_long.predict( X_tt )
ym_lat = lm_lat.predict( X_tt )

# mean square error
mse_long = metrics.mean_squared_error( y_tt[:, 0], ym_long, squared=False )
mse_lat = metrics.mean_squared_error( y_tt[:, 1], ym_lat, squared=False )

print( "MSE longitude: ", mse_long )
print( "MSE latitude: ", mse_lat )

"""### Confronto visivo diretto per SVR"""

fig2, ax2 = plt.subplots( ncols = 2 )

fig2.suptitle( "Comparison original map Vs.machine output" )

# il primo plot mostra il test set originale
ax2[0].set_title( "Real output" )
ax2[0].grid( True )
ax2[0].plot( y_tt[:, 0], y_tt[:, 1], 'xr' )
ax2[0].set_xlabel( "longitude" )
ax2[0].set_ylabel( "latitude" )

# il secondo plot mostra invece l'output della macchina
ax2[1].set_title( "Machine output" )
ax2[1].grid( True )
ax2[1].plot( ym_long, ym_lat, 'ob' )
ax2[1].set_xlabel( "longitude" )
ax2[1].set_ylabel( "latitude" )

plt.show( )

"""### Scatter Plots per SVR"""

fig3, ax3 = plt.subplots( ncols = 2 )

sc_minmax = list() 
sc_minmax.append( [np.min(y_tt[:, 0]), np.max(y_tt[:, 0])] )
sc_minmax.append( [np.min(y_tt[:, 1]), np.max(y_tt[:, 1])] )
print( sc_minmax )

fig2.suptitle( "Scatter Plots for Longitude and Latitude" )

# scatter plot per la longitudine
ax3[0].set_title( "Longitude" )
ax3[0].grid( True )
ax3[0].plot( sc_minmax[0], sc_minmax[0], '--b' )
ax3[0].plot( y_tt[:, 0], ym_long, 'xr' )
ax3[0].set_xlabel( "real longitude" )
ax3[0].set_ylabel( "predicted longitude" )

# scatter plot per la latitudine
ax3[1].set_title( "Latitude" )
ax3[1].grid( True )
ax3[1].plot( sc_minmax[1], sc_minmax[1], '--b' )
ax3[1].plot( y_tt[:, 1], ym_lat, 'xr' )
ax3[1].set_xlabel( "real latitude" )
ax3[1].set_ylabel( "predicted latitude" )

plt.show( )

"""salvataggio dei modelli, vedi [qui](https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/). """

'''
print( "--- Longitude ---" )
print( "n support vectors : ", lm_long.n_support_, " out of ", X_tr.shape[0], " samples" )
# print( "weights: ", lm_long.class_weight_ )
print( "dual_coef: ", lm_long.dual_coef_ )
# print( "support vectors: ", lm_long.class_weight_ )
print( "indices of the support vectors: ", lm_long.support_ )
'''

# print di tutti i support vectors
'''
for i in range(0, len(lm_long.dual_coef_[0])):
  print( "no.", i, " weigth (", lm_long.dual_coef_[0][i], ") sample: ", X_tr[i, :] )

print( "intercept : ", lm_long.intercept_ )
print( "---" )

# salvataggio dei dual coeff. in CSV
type(lm_long.dual_coef_)
df_coeff = pd.DataFrame(lm_long.dual_coef_)
df_coeff.to_csv("LM_coefficients.csv")

# salvataggio support 
df_support = pd.DataFrame(lm_long.support_)
df_support.to_csv("LM_support_indexes")
'''

if save_res:
  with open( save_path + 'long_data.sav', 'wb' ) as fil:
    pk.dump( lm_long, fil )

  with open( save_path + 'lat_data.sav', 'wb' ) as fil:
    pk.dump( lm_lat, fil )

"""# Coefficienti learning"""

# print( LM_long.C )
# print( LM_lat )
LM_long = lm_long
LM_lat = lm_lat

figt, axt = plt.subplots( nrows=2 )

figt.suptitle( "Alpha Coefficients of SVR" )

axt[0].grid( True )
axt[0].plot( [-10, X_tr.shape[0]*1.05], [LM_long.C, LM_long.C], '--y' )
axt[0].plot( [-10, X_tr.shape[0]*1.05], [-LM_long.C, -LM_long.C], '--y' )
axt[0].set_title( f"Longitude ({LM_long.dual_coef_.shape[1]} samples out of {X_tr.shape[0]} trainig samples -- C={LM_lat.C} eps={LM_lat.epsilon})" )
axt[0].plot( np.array( range( 0, LM_long.dual_coef_.shape[1] ) ), LM_long.dual_coef_[0, :], 'xb' )

axt[1].grid( True )
axt[1].plot( [-10, X_tr.shape[0]*1.05], [LM_lat.C, LM_lat.C], '--y' )
axt[1].plot( [-10, X_tr.shape[0]*1.05], [-LM_lat.C, -LM_lat.C], '--y' )
axt[1].set_title( f"Latitude ({LM_lat.dual_coef_.shape[1]} samples out of {X_tr.shape[0]} trainig samples -- C={LM_long.C} eps={LM_long.epsilon})" )
axt[1].plot( np.array( range( 0, LM_lat.dual_coef_.shape[1] ) ), LM_lat.dual_coef_[0, :], 'xb' )

plt.show( )